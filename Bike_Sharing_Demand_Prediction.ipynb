{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukilearn/CAPSTONE_2_Bike_Sharing_Demand_Prediction_ML_Regression/blob/main/Bike_Sharing_Demand_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - REGRESSION\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name -** G MOHAMED LUQMAN\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime as dt\n",
        "\n",
        "\n",
        "# Import Visualization Libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Import warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import preporcessing libraries\n",
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
        "\n",
        "# Import model selection libraries\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
        "\n",
        "# Import Outlier influence library\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Import Model\n",
        "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "# Import evaluation metric libraries\n",
        "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error\n",
        "\n",
        "# Import tree for visualization\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn import tree\n",
        "from IPython.display import SVG,display\n",
        "from graphviz import Source"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load Dataset from github repository\n",
        "url = 'https://raw.githubusercontent.com/lukilearn/CAPSTONE_2_Bike_Sharing_Demand_Prediction_ML_Regression/main/SeoulBikeData.csv'\n",
        "data = pd.read_csv(url, encoding='unicode_escape')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "# View top 5 rows of the dataset\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View last 5 rows of the dataset\n",
        "\n",
        "data.tail()"
      ],
      "metadata": {
        "id": "85YihF0InnMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "# Checking number of rows and columns of the dataset using shape\n",
        "print(\"Number of rows are: \",data.shape[0])\n",
        "print(\"Number of columns are: \",data.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "data.info()\n",
        "\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values, Missing Values/Null Values, Unique value"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# This user define function will give the type,count of null and non null values as well as null ratio\n",
        "\n",
        "def bike_info():\n",
        "  temp=pd.DataFrame(index=data.columns)\n",
        "  temp[\"datatype\"]=data.dtypes\n",
        "  temp[\"not null values\"]=data.count()\n",
        "  temp[\"null value\"]=data.isnull().sum()\n",
        "  temp[\"% of null value\"]=data.isnull().mean()*100\n",
        "  temp[\"unique count\"]=data.nunique() # Return Series with number of distinct elements. Can ignore NaN values.\n",
        "  return temp\n",
        "bike_info()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "import missingno as msno\n",
        "msno.bar(data, color='green',sort='ascending', figsize=(10,2), fontsize=15)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing the missing values using Heatmap\n",
        "plt.figure(figsize=(12,2))\n",
        "sns.heatmap(data.isna(), cmap = 'coolwarm')"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a total of 14 feature columns where Rented Bike Count is the dependent variable column. The total number of observations(rows) are 8760.\n",
        "\n",
        "There are no duplicate rows in the dataset.\n",
        "\n",
        "Also there are no missing values or Null values in the dataset.\n",
        "\n",
        "Dataset has all unique values i.e., there is no duplicate, which means data is free from bias as duplicates which can cause problems in downstream analysis, such as biasing results or making it difficult to accurately summarize the data.\n",
        "Date has some object data types, it should be datetime data type."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "print(f'Features: {data.columns.to_list()}')"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Looking for the description of the dataset to get insights of the data\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The dataset contains weather information (Temperature, Humidity, Windspeed, Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "    Date : year-month-day\n",
        "    Rented Bike count - Count of bikes rented at each hour\n",
        "    Hour - Hour of the day\n",
        "    Temperature-Temperature in Celsius\n",
        "    Humidity - %\n",
        "    Windspeed - m/s\n",
        "    Visibility - 10m\n",
        "    Dew point temperature - Celsius\n",
        "    Solar radiation - MJ/m2\n",
        "    Rainfall - mm\n",
        "    Snowfall - cm\n",
        "    Seasons - Winter, Spring, Summer, Autumn\n",
        "    Holiday - Holiday/No holiday\n",
        "    Functional Day - NoFunc(Non Functional Hours), Fun(Functional hours)\n",
        "\n",
        "### Observations:\n",
        "\n",
        "    We are focusing on several key columns of our dataset, including 'Hour', 'Holiday', 'Functioning Day', 'Rented Bike Count', 'Temperature(°C)', and 'Seasons', as they contain a wealth of information.\n",
        "    By utilizing these features, we plan to create a regression model and implement various regression algorithms.\n",
        "    There is a column 'Hour' which might be considered a categorical feature or maybe a numerical feature based on the data we will try both and see the result difference.\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check Unique Values for each variable using a for loop\n",
        "for i in data.columns.tolist():\n",
        "  print(\"No. of unique values in\",i,\"is\",data[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a some new features:\n",
        "\n",
        "# Renaming complex columns name for the sake of simplicity\n",
        "data=data.rename(columns={'Rented Bike Count':'rented_bike_count',\n",
        "                                'Date':'date',\n",
        "                                'Hour':'hour',\n",
        "                                'Seasons':'seasons',\n",
        "                                'Holiday':'holiday',\n",
        "                                'Temperature(°C)':'temperature',\n",
        "                                'Humidity(%)':'humidity',\n",
        "                                'Wind speed (m/s)':'wind_speed',\n",
        "                                'Visibility (10m)':'visibility',\n",
        "                                'Dew point temperature(°C)':'dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'solar_radiation',\n",
        "                                'Rainfall(mm)':'rainfall',\n",
        "                                'Snowfall (cm)':'snowfall',\n",
        "                                'Functioning Day':'func_day'})"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "1km1tMoR5Lbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "kdY_aMa96Cbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Date into year, month, day & day_name\n",
        "data.date = pd.to_datetime(data.date)\n",
        "\n",
        "data['day'] = data['date'].dt.day\n",
        "data['month'] =data['date'].dt.month\n",
        "data['year'] = data['date'].dt.year\n",
        "data['weekday'] = data['date'].dt.day_name()\n",
        "\n",
        "# droping Date column\n",
        "data.drop('date', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "qZSCIdiE5zHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.hour.unique()"
      ],
      "metadata": {
        "id": "wSYO91SA7X4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hours of the day follow a clear sequence, with 9 am being closer to 10 am than it is to 8 am, and farther from 6 pm. This feature can be classified as a discrete ordinal variable. We will consider the hour as a categorical value and transform it into a numerical value to see if there is any difference in the results.\n"
      ],
      "metadata": {
        "id": "eYau05qd8YUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def session(x):\n",
        "\n",
        "    '''\n",
        "    For exploratory data analysis (EDA) purposes, the \"Hour\" column can be converted into categorical variables\n",
        "    such as \"Morning\", \"Noon\", and \"Night\", without altering the existing label encoding format of the \"Hour\" column.\n",
        "    This conversion is not necessary for model training.\n",
        "    '''\n",
        "\n",
        "    if x>4 and x<=8:\n",
        "        return 'Early Morning'\n",
        "    elif x>8 and x<=12:\n",
        "        return 'Morning'\n",
        "    elif x>12 and x<=16:\n",
        "        return 'Afternoon'\n",
        "    elif x>16 and x<=20:\n",
        "        return 'Evening'\n",
        "    elif x>20 and x<=24:\n",
        "        return 'Night'\n",
        "    elif x<=4:\n",
        "        return 'Late Night'\n",
        "\n",
        "#apply funtion to make new category column\n",
        "data['session'] = data['hour'].apply(session)"
      ],
      "metadata": {
        "id": "2vfxhPOe8pTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining continuous independent variables separately\n",
        "data_var = ['temperature', 'humidity', 'wind_speed', 'visibility','dew_point_temperature','solar_radiation', 'rainfall', 'snowfall']"
      ],
      "metadata": {
        "id": "1z9D0eJt95Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining dependent variable\n",
        "dependent_variable = ['rented_bike_count']\n",
        "\n",
        "# defining categorical independent variables separately\n",
        "cat_var = ['hour','seasons', 'holiday', 'func_day', 'month', 'weekday']"
      ],
      "metadata": {
        "id": "ZojyzLeZ8t_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_var\n"
      ],
      "metadata": {
        "id": "p6vkmGNEQ0rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "p97hJ_WFCUA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the Date column, 'month' and 'day of the week' columns are created.\n",
        "\n",
        "To capture this trend, we can define a new feature 'weekend' which indicates whether a said day is a weekend (1) or not (0).\n",
        "\n",
        "From the day of the week column, weekend column is created where 6 and 7 are the weekends (Saturday and Sunday).\n",
        "\n",
        "We have also defined the continuous variables, dependent variable and categorical variables for ease of plotting graphs."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploratory Data Analysis Of The Data Set**"
      ],
      "metadata": {
        "id": "RQDi-RM9ChRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do we perform EDA?**\n",
        "* ***An EDA is a thorough examination meant to uncover the underlying structure of a data set and is important for a company because it exposes trends, patterns, and relationships that are not readily apparent.***"
      ],
      "metadata": {
        "id": "E-24UCZPCjAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do you do univariate analysis?**\n",
        "\n",
        "* ***The key objective of Univariate analysis is to simply describe the data to find patterns within the data.***"
      ],
      "metadata": {
        "id": "k2drARGRCoPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do you do Bivariate analysis?**\n",
        "* ***The key objective of bivariate analysis is to explore and understand the relationship or association between two variables***"
      ],
      "metadata": {
        "id": "EhCV3qhsC1iU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do you do multivariate analysis?**\n",
        "* ***The key objective of multivariate analysis is to study and understand the relationships, patterns, and interactions among multiple variables simultaneously.***"
      ],
      "metadata": {
        "id": "x3jXDOD_E4f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 : Dependent variable Distribution"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart-1 Visualization code for distribution of target variable\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.distplot(data['rented_bike_count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1.Why did you pick the specific chart?**"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A distplot, also known as a histogram-kernel density estimate (KDE) plot. It is useful because it provides a quick and easy way to check the distribution of the data, identify patterns or outliers, and compare the distribution of multiple variables. It also allows to check if the data is following normal distribution or not.\n",
        "\n",
        "Thus, I used the histogram plot to analyse the variable distributions over the whole dataset whether it's symmetric or not."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. **What is/are the insight(s) found from the chart?**"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above distribution plot of dependent variable rented bike, we can clearly see that the distribution is positively skewed (Right skewed).\n",
        "\n",
        "It means that distribution is not symmetric around the the mean.\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. **Will the gained insights help creating a positive business impact?**\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, definately from this insight we got to know that we our data is not normally distributed so, before doing or implementing any model on this data we need to normalise this data."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2  Distribution/ Box plot"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 : Visualizing code of histogram plot & boxplot for each columns to know the data distribution\n",
        "for col in data.describe().columns:\n",
        "    fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(18,6))\n",
        "    sns.histplot(data[col], ax = axes[0],kde = True)\n",
        "    sns.boxplot(data[col], ax = axes[1],orient='h',showmeans=True,color='pink')\n",
        "    fig.suptitle(\"Distribution plot of \"+ col, fontsize = 15)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1. Why did you pick the specific chart?**"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of chart that displays the distribution of a dataset. It is a graphical representation of the data that shows how often each value or group of values occurs. Histplots are useful for understanding the distribution of a dataset and identifying patterns or trends in the data. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, we used the histogram plot to analysis the variable distributions over the whole dataset whether it's symmetric or not.\n",
        "\n",
        "A boxplot is used to summarize the key statistical characteristics of a dataset, including the median, quartiles, and range, in a single plot. Boxplots are useful for identifying the presence of outliers in a dataset, comparing the distribution of multiple datasets, and understanding the dispersion of the data. They are often used in statistical analysis and data visualization.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, we used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. **What is/are the insight(s) found from the chart?**"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above univariate analysis of all continuous feature variables. We got to know that only tempture and humidity columns are looks normally distributed others shows the different distributions.\n",
        "\n",
        "Also we can see that there are outlier values in snowfall, rainfall, wind speed & solar radiation columns\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. **Will the gained insights help creating a positive business impact?**\n",
        "\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram and Box plot cannot give us whole information regarding data. It's done just to see the distribution of the column data over the dataset."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : Dependent variable with continuous variables (Bivariate)"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3\n",
        "\n",
        "# Analyzing the relationship between the dependent variable and the continuous variables\n",
        "for i in data_var:\n",
        "  plt.figure(figsize=(11,8))\n",
        "  sns.regplot(x=i,y=dependent_variable[0],data=data)\n",
        "  plt.xlabel(i)\n",
        "  plt.ylabel(dependent_variable[0])\n",
        "  plt.title(i+' vs '+ dependent_variable[0])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1. Why did you pick the specific chart?**"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regplot is used to create a scatter plot with linear regression line. The purpose of this function is to visualize the relationship between two continuous variables. It can help to identify patterns and trends in the data, and can also be used to test for linearity and independence of the variables.\n",
        "\n",
        "To check the patterns between independent variable with our rented bike dependent variable we used this regplot.\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2. What is/are the insight(s) found from the chart?**"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above regression plot we can see that there is some linearity between temperature, solar radiation & dew point temperature with dependent variable rented bike\n",
        "\n",
        "Other variables are not showing any patterns.\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. **Will the gained insights help creating a positive business impact?**\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, it helped a little bit from this we got to know that there are few variables which are showing some patterns with dependent variable this variable are maybe important feature while predicting for rented bike count so business needs focus on these variables."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Chart - 4 : Categorical variables with dependent variable (bivariate)**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Analyzing the relationship between the dependent variable and the categorical variables\n",
        "for i in cat_var:\n",
        "  plt.figure(figsize=(11,8))\n",
        "  sns.barplot(x=i,y=dependent_variable[0],data=data)\n",
        "  plt.xlabel(i)\n",
        "  plt.ylabel(dependent_variable[0])\n",
        "  plt.title(i+' vs '+ dependent_variable[0])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1. Why did you pick the specific chart?**"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n",
        "\n",
        "To show the distribution of the rented bike count with other categorical variables we used bar charts.\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2. What is/are the insight(s) found from the chart?**"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above bar charts we got insights:\n",
        "\n",
        "1. In hour vs rented bike chart there is high demand in the morning 8'o clock and evening 18'o clock\n",
        "2. From season vs rented bike chart there is more demand in summer and less demand in winter.\n",
        "3. There is high demand on working days.\n",
        "4. From month chart we know that there is high demand in month of june.\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3. Will the gained insights help creating a positive business impact?**\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, this insights are going to provide some positive business impact, beacause analysing the demand on the basis of categorical varible we got know that when demand for bike is more so we can focus more on that portion.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 : Rented Bike vs Hour"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "#ploting line graph\n",
        "# group by Hrs and get average Bikes rented, and precent change\n",
        "avg_rent_hrs = data.groupby('hour')['rented_bike_count'].mean()\n",
        "\n",
        "# plot average rent over time(hrs)\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.lineplot(data=avg_rent_hrs, marker='o')\n",
        "plt.title('Average bike rented per hour')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1. Why did you pick the specific chart?**"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot, also known as a line chart or line graph, is a way to visualize the trend of a single variable over time. It uses a series of data points connected by a line to show how the value of the variable changes over time.\n",
        "\n",
        "Line plots are useful because they can quickly and easily show trends and patterns in the data. They are particularly useful for showing how a variable changes over a period of time. They are also useful for comparing the trends of multiple variables.\n",
        "\n",
        "To see how rented bike demand is distributed over 24 hours time we used line plot.\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2. What is/are the insight(s) found from the chart?**"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above line plot we can clearly see that there is high demand in the morning and in the evening."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3. Will the gained insights help creating a positive business impact?**\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, from above insight we know that there is high demand in morning and evening so business needs to focus more on that time slot. as well as try to meet the demand on that time slot."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Chart - 6 : Bike demand throughout the day (Multivariate)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "for i in cat_var:\n",
        "  if i == 'hour':\n",
        "    continue\n",
        "  else:\n",
        "    fig, ax = plt.subplots(figsize=(12,8))\n",
        "    sns.pointplot(data=data, x='hour', y='rented_bike_count', hue=i, ax=ax)\n",
        "    plt.title('Hourly bike demand broken down based on the attribute: '+i)\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left',title=i)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1. Why did you pick the specific chart?**"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line plot, also known as a line chart or line graph, is a way to visualize the trend of a single variable over time. It uses a series of data points connected by a line to show how the value of the variable changes over time.\n",
        "\n",
        "Line plots are useful because they can quickly and easily show trends and patterns in the data. They are particularly useful for showing how a variable changes over a period of time. They are also useful for comparing the trends of multiple variables.\n",
        "\n",
        "To show the demand of rented bike throughout the day on the basis of other categorical variable we used line plot drawing multiple lines on charts."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2. What is/are the insight(s) found from the chart?**"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above line plots we see that :\n",
        "\n",
        "1. In winter season there is no significant demand even in the morning or in the evening.\n",
        "2. On the functional day (i.e No Holiday) there is spike in morning and in evening, but that is not there on Holidays.\n",
        "3. Around 3 months in winter season (i.e December, January & February) there is low demand.\n",
        "4. On weekend almost throught the day there is demand.\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **3. Will the gained insights help creating a positive business impact?**\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "Yes, from this analysis we figure out some key factors such as high demand in morning and evening slot in all the seasons."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 : Categorical plot for seasons"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#plot for rented bike count seasonly\n",
        "sns.catplot(x='seasons',y='rented_bike_count',data=data)"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Catplot is used to create a categorical plot. Categorical plots are plots that are used to visualize the distribution of a categorical variable. They can be used to show how a variable is related to a categorical variable and can also be used to compare the distribution of multiple categorical variables.\n",
        "\n",
        "To see the distribution of the rented bike on basis of season column we used catplot."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above catplot we got know that:\n",
        "\n",
        "1. There is low demand in winter\n",
        "2. Also in all seasons upto the 2500 bike counts distribution is seen dense.\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, from this catplot we know that there is high bike count upto the 2500 so, above that there maybe outliers present. business needs to evaluate that."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Chart - 8 visualization code\n",
        "Winter=data[data[\"seasons\"]=='Winter'].sum()\n",
        "Spring=data[data[\"seasons\"]=='Spring'].sum()\n",
        "Summer=data[data[\"seasons\"]=='Summer'].sum()\n",
        "Autumn=data[data[\"seasons\"]=='Autumn'].sum()\n",
        "\n",
        "BikeSeasons={\"Winter\":Winter[\"rented_bike_count\"],\"Spring\":Spring[\"rented_bike_count\"],\"Summer\":Summer[\"rented_bike_count\"],\"Autumn\":Autumn[\"rented_bike_count\"]}\n",
        "plt.gcf().set_size_inches(10,10)\n",
        "plt.pie(BikeSeasons.values(),labels=BikeSeasons.keys(), autopct='%1d%%');\n",
        "plt.title(\"Repartition of bikes rental by season\", fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie charts are generally used to show the proportions of a whole, and are especially useful for displaying data that has already been calculated as a percentage of the whole.\n",
        "\n",
        "So, we used pie chart to see percentage distribution of rented bike on the basis of sseasons"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above pie chart:\n",
        "\n",
        "1. In year data season summer contributes around 36% then autumn around 29%\n",
        "2. Lowest demand in winter, it contributes around only 7%"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This insights only tell about percentage contribution of year data of season varible, which clearly gave indication about demand."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "corr = data.corr()\n",
        "mask = np.zeros_like(corr)\n",
        "\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "with sns.axes_style(\"white\"):\n",
        "    f, ax = plt.subplots(figsize=(18, 9))\n",
        "    ax = sns.heatmap(corr , mask=mask, vmin = -1,vmax=1, annot = True, cmap=\"YlGnBu\")"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient is a measure of the strength and direction of a linear relationship between two variables. A correlation matrix is used to summarize the relationships among a set of variables and is an important tool for data exploration and for selecting which variables to include in a model. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, we have used correlation heatmap.\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above correlation map we can clearly see that:\n",
        "\n",
        "1. There is high multicolinearity between independent variable (i.e temperature & dew point temp, humidity & dew point temp, weekend & day of week).\n",
        "2. There is correlation of temperature, hour, dew point temp & solar radiation with dependent variable rented bike.\n",
        "3. Other than that we didnt see any correlation."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 - Pair Plot"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# Pair Plot\n",
        "sns.pairplot(data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n",
        "\n",
        "Thus, we used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above pair plot we got to know that, there is not clear linear relationship between variables. other than dew point temp, temperature & solar radiation there is not any reationship."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on above chart experiments we have noticed that our dependent variable does not seems to normally distributed so we have made hypothetical assumption that our data is normally distributed and for that we have decided to do statistical analysis."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normality test\n",
        "\n",
        "for normality test we decided\n",
        "1. Null hypothesis : Data is normally distributed\n",
        "2. Alternate hypothesis : Data is not normally distributed"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import shapiro\n",
        "test_data = data['rented_bike_count']\n",
        "\n",
        "stats,p  = shapiro(test_data)\n",
        "print('stats = %.2f, p=%.3f' % (stats,p))\n",
        "\n",
        "if p <= 0.05:\n",
        "  print(\"Reject null hypothesis i.e  Data is not normally distributed\")\n",
        "else:\n",
        "  print('Accept null hypothesis i.e  Data is normally distributed')"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Shapiro-wilk statistical test to obtain the p-value and we got very less p-value which is less than 0.05."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Shapiro-Wilk test is used to test the normality of a sample. The test checks whether the sample data fits a normal distribution, which is often assumed for statistical analysis. The test results can help determine if the data should be transformed or if non-parametric statistical methods should be used instead of traditional parametric methods."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "What is data cleaning?\n",
        "\n",
        "Data cleaning is the process of identifying and correcting or removing inaccuracies, inconsistencies, and missing values in a dataset. It is an important step in the data preparation process that ensures that the data is accurate, complete, and in a format that can be easily analyzed. Data cleaning may include tasks such as removing duplicate records, filling in missing values, correcting errors, and standardizing data formats. The goal of data cleaning is to improve the quality of the data and make it suitable for further analysis and modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "lzGwQYZqyiJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Duplicate Values\n"
      ],
      "metadata": {
        "id": "2X_dfBs-yl7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# counting duplicate values\n",
        "data.duplicated().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "tFuYrErCyp9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Missing Values/Null Values Count\n",
        "print(data.isnull().sum())\n",
        ""
      ],
      "metadata": {
        "id": "M4oXtNJNy3MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Skewness"
      ],
      "metadata": {
        "id": "xSNnJcAwzIcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# statistical summary\n",
        "data.describe().T\n",
        "\n"
      ],
      "metadata": {
        "id": "omVrJEZGzNP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "As can be seen in the statistical summary for numerical features, there is a significant difference between the 75% percentile and maximum value, indicating that the dataset contains skewness and outliers.\n"
      ],
      "metadata": {
        "id": "rd3grqWvzeIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing rainfall and snowfall as it may remove important information as these 2 columns are highly skewed.\n",
        "data_var.remove('rainfall')\n",
        "data_var.remove('snowfall')"
      ],
      "metadata": {
        "id": "MuMiDFX9KYUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(15,6))\n",
        "# title\n",
        "plt.suptitle('Data Distibution of Numerical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(data_var):\n",
        "  plt.subplot(3,3,i+1)\n",
        "\n",
        "  # dist plots\n",
        "  sns.distplot(data[col])\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wn6YudRF3Rn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Observation:\n",
        "\n",
        "1.  For numerical features, we can see that the majority of distributions are right-skewed. The distribution of rainfall, snowfall, and solar radiation is highly skewed to the right. It demonstrates that these columns have many outliers. Some columns are negatively skewed.\n",
        "2.   Some of the variables can get a normal distribution when outliers are removed. As a result, it appears that outliers should be removed before the transformation. First, we will get rid of outliers, and then we check to see if we need to use the transformation technique again.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KRs_yeZc3xIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# figsize\n",
        "plt.figure(figsize=(15,5))\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numerical Features', fontsize=20, fontweight='bold', y=1)\n",
        "\n",
        "for i,col in enumerate(data_var):\n",
        "  plt.subplot(3,3,i+1)\n",
        "\n",
        "  # countplot\n",
        "  sns.boxplot(data[col],orient=\"h\")\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()\n",
        ""
      ],
      "metadata": {
        "id": "3Cge6aUc2eD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Observation:\n",
        "\n",
        "\n",
        "1.   Outliers are visible in the 'wind_speed','solar_radiation','rainfall',and 'snowfall' columns.\n",
        "    Since we have limited datapoint hence we are not simply removing the outlier instead of that we are using the clipping method\n",
        "2.   Clipping Method: In this method, we set a cap on our outliers data, which means that if a value is higher than or lower than a certain threshold, all values will be considered outliers. This method replaces values that fall outside of a specified range with either the minimum or maximum value within that range.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tJEbFV8Z4Wdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# we are going to replace the datapoints with upper and lower bound of all the outliers\n",
        "\n",
        "def clip_outliers(data):\n",
        "    for col in data[data_var]:\n",
        "        # using IQR method to define range of upper and lower limit.\n",
        "        q1 = data[col].quantile(0.25)\n",
        "        q3 = data[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "        # replacing the outliers with upper and lower bound\n",
        "        data[col] = data[col].clip(lower_bound, upper_bound)\n",
        "    return data\n",
        "\n"
      ],
      "metadata": {
        "id": "tR-0JDD54yLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# using the function to treat outliers\n",
        "data = clip_outliers(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "B1-P5Ok95E4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the boxplot after outlier treatment\n",
        "\n",
        "# figsize\n",
        "plt.figure(figsize=(15,5))\n",
        "# title\n",
        "plt.suptitle('Outlier Analysis of Numerical Features', fontsize=20, fontweight='bold', y=1.02)\n",
        "\n",
        "for i,col in enumerate(data_var):\n",
        "  plt.subplot(3,3, i+1)\n",
        "\n",
        "  # countplot\n",
        "  sns.boxplot(data[col], orient='h')\n",
        "  # x-axis label\n",
        "  plt.xlabel(col)\n",
        "  plt.tight_layout()\n",
        ""
      ],
      "metadata": {
        "id": "ttA4GVNF5LYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use IQR method and Capping method, Based on IQR method we set Upper limit and Lower limit of rented bike count and convert those outliers into median values.\n",
        "\n",
        "Also we capp outliers upto 99th percentile and above that we convert those outliers into upper limit value"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note :-\n",
        "\n",
        "1. We have tried to remove the outliers but it is seen that there is drop in performance after removing the outliers around 10% drop in model performance\n",
        "\n",
        "So, we have decided that we will perform the model without removing the outliers."
      ],
      "metadata": {
        "id": "JM91ed077MBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.Feature *Manipulation*"
      ],
      "metadata": {
        "id": "40oaUxfiD6E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dew point temperature"
      ],
      "metadata": {
        "id": "ns-lIK-yLb_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We know that the dew point temperature is related to temperature and can be approximated as follows:\n",
        "\n",
        "Td = T - ((100 - RH)/5.)\n",
        "\n",
        "Where\n",
        "\n",
        "    Td - Dew point temperature (in degrees Celsius)\n",
        "    T - Observed temperature (in degrees Celsius)\n",
        "    RH - Relative humidity (in percent)\n"
      ],
      "metadata": {
        "id": "-lIYjT3kLU4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Scatter plot to visualize the relationship between\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.scatter(x='temperature',y='dew_point_temperature',data=data )\n",
        "plt.xlabel('temperature')\n",
        "plt.ylabel('dew_point_temp')\n",
        "plt.title('Temperature VS Dew point Temperature')\n",
        "\n"
      ],
      "metadata": {
        "id": "Xjn4pQxxLgiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# correlation\n",
        "data[['temperature','dew_point_temperature']].corr()\n",
        "\n"
      ],
      "metadata": {
        "id": "uk2xxHhZL3Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observation:\n",
        "\n",
        "\n",
        "    The temperature and dew point temperature are highly correlated (0.912).\n",
        "    Also, from the above scatter plot, it is clear that as the temperature increases, the dew point temperature also increases.\n",
        "    Hence we can drop the column from the dataset since it will not increase the accuracy of predictions, and will only increase the model complexity.\n",
        "\n"
      ],
      "metadata": {
        "id": "sELILlNNMElT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# dropping dew point temperature\n",
        "data.drop('dew_point_temperature', axis=1,inplace=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZwBrgpNbMLS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.Feature Selection"
      ],
      "metadata": {
        "id": "wByFoWoqEot_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **VIF**\n",
        "\n",
        "A variance inflation factor(VIF) detects multicollinearity in regression analysis. Multicollinearity is when there’s correlation between predictors (i.e. independent variables) in a model; it’s presence can adversely affect your regression results. The VIF estimates how much the variance of a regression coefficient is inflated due to multicollinearity in the model."
      ],
      "metadata": {
        "id": "7lyI-IuFEw-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calc_vif(X):\n",
        "\n",
        "  # For each X, calculate VIF and save in dataframe\n",
        "  vif = pd.DataFrame()\n",
        "  vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "  vif[\"features\"] = X.columns\n",
        "\n",
        "  return vif"
      ],
      "metadata": {
        "id": "EMHIB4sj6nqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(data[[i for i in data.describe().columns]])"
      ],
      "metadata": {
        "id": "elIdVP0vFPLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the VIF factor of 'year' is too large hence we will remove the year from our data to build our model."
      ],
      "metadata": {
        "id": "t2I2CjYpLp-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# droping year column due to multi-collinearity\n",
        "\n",
        "data.drop('year', axis=1, inplace=True)\n",
        "\n",
        "# dropping columns those were created for EDA purpose only and do not account for any information addition\n",
        "data.drop('weekday', axis=1, inplace=True)         # day column is already present with ordinal numbering\n",
        "data.drop('session', axis=1, inplace=True)         # hour column is already present with ordinal numbering"
      ],
      "metadata": {
        "id": "xXeYue2oLu0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the VIF again\n",
        "calc_vif(data[[i for i in data.describe().columns]])"
      ],
      "metadata": {
        "id": "GmXUURx-L3M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# droping func_day column due to multi-collinearity\n",
        "\n",
        "data.drop('func_day', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "-uaCjSauMkiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the VIF again\n",
        "calc_vif(data[[i for i in data.describe().columns]])"
      ],
      "metadata": {
        "id": "5zq0PxzVMsuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?\n"
      ],
      "metadata": {
        "id": "2Jz9jPjeM-9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used pearson correlation coefficient to check correlation between variables and also with dependent variable\n",
        "\n",
        "And also we check the multicolinearity using VIF and remove those who are having high VIF value."
      ],
      "metadata": {
        "id": "jhqCeZzzR0cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Encoding**\n",
        "\n",
        "Encoding is a technique in feature engineering that is used to convert categorical variables into numerical values that can be used by machine learning algorithms.\n",
        "\n",
        "There are several encoding techniques, including:\n",
        "\n",
        "1. One-hot encoding: creates a binary column for each unique category, with a value of 1 indicating the presence of the category and 0 indicating the absence.\n",
        "2. Label encoding: assigns a unique integer value to each category.\n",
        "3. Ordinal encoding: assigns an ordered integer value to each category based on the natural ordering of the categories.\n",
        "4. Count encoding: replaces a categorical value with the number of times it appears in the dataset.\n"
      ],
      "metadata": {
        "id": "D4ht3v5T-MLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting snowfall and rainfall to categorical attributes\n",
        "data['snowfall'] = data['snowfall'].apply(lambda x: 1 if x>0 else 0)\n",
        "data['rainfall'] = data['rainfall'].apply(lambda x: 1 if x>0 else 0)"
      ],
      "metadata": {
        "id": "gr7jXXMhHav9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding the visibility column\n",
        "data['visibility'] = data['visibility'].apply(lambda x: 0 if 0<=x<=399 else (1 if 400<=x<=999 else 2))"
      ],
      "metadata": {
        "id": "es3Wmuv8Hc8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding\n",
        "data['holiday'] = np.where(data['holiday'] == 'Holiday', 1,0)"
      ],
      "metadata": {
        "id": "NhqB5AWsHiyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_var"
      ],
      "metadata": {
        "id": "h1Wo0IEsN3bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One Hot Encoding\n",
        "\n",
        "# One-hot encode the 'seasons' feature\n",
        "seasons_onehot = pd.get_dummies(data['seasons'], prefix='seasons')\n",
        "\n",
        "# drop the original features\n",
        "data.drop('seasons', axis=1, inplace=True)\n",
        "\n",
        "# concatenate the one-hot encoded season feature with the rest of the data\n",
        "data = pd.concat([data, seasons_onehot], axis=1)"
      ],
      "metadata": {
        "id": "zOLWUa5tOt6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are very few day on which there was snowfall / rainfall, it is in our interest that we convert these columns to binary categorical columns indicating whether there was rainfall / snowfall at that particular hour\n",
        "\n",
        "For visibility\n",
        "\n",
        "When\n",
        "\n",
        "Visibility >= 20 Km ---> Clear (high visibility)\n",
        "\n",
        "4 Km <= Visibility < 10 Km ---> Haze (medium visibility)\n",
        "\n",
        "Visibility < 4 Km ---> Fog (low visibility)\n",
        "\n",
        "Converting visibility based on the above mentioned threshold values. Since they are ordinal, we can encode them as 0 (low visibility), 1 (medium visibility), 2 (high visibility)\n",
        "\n",
        "\n",
        "For seasons, we use here one hot encoding\n"
      ],
      "metadata": {
        "id": "cFCXlmH-M_Dz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalization of Target Variable**\n",
        "\n",
        "A fundamental component of data mining is data normalization, also known as data pre-processing. It refers to transforming the data, specifically converting the source data into a different format that makes it possible to effectively process the data. Data normalization's primary goal is to reduce or even eliminate duplicate data"
      ],
      "metadata": {
        "id": "Mv9r2LK4SlrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2 , figsize = (15,5))\n",
        "\n",
        "# Distribution plot of Rented Bike Count\n",
        "dist =sns.distplot(data['rented_bike_count'],hist=True, ax = ax[0])\n",
        "dist.set(xlabel = 'Rented Bike Count', ylabel ='Density', title = 'Distribution Plot of Target Variable')\n",
        "\n",
        "# mean line(blue)\n",
        "dist.axvline(data['rented_bike_count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "# median line(black)\n",
        "dist.axvline(data['rented_bike_count'].median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# Boxplot\n",
        "box = sns.boxplot(data.rented_bike_count, ax= ax[1], orient='h')\n",
        "box.set(title = 'Outlier Analysis of Target Variable')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "\n",
        "* The graph above indicates that the Rented Bike Count has a moderate right skewness. Linear regression assumes that the dependent variable has a normal distribution, therefore, to meet this assumption, we need to take some measures to normalize the distribution.\n",
        "* The boxplot above indicates that there are outliers in the rented bike count column."
      ],
      "metadata": {
        "id": "fyfOsxfbS553"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the best transformation for our target variable\n",
        "fig, axs = plt.subplots(1,3, figsize=(16,4))\n",
        "\n",
        "sns.distplot(np.log1p(data['rented_bike_count']),kde=True, ax=axs[0])\n",
        "sns.distplot(np.sqrt(data['rented_bike_count']),kde=True, ax=axs[1])\n",
        "sns.distplot(np.cbrt(data['rented_bike_count']),kde=True, ax=axs[2])"
      ],
      "metadata": {
        "id": "OiyMnN4QTHXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations\n",
        "\n",
        "* Applying a logarithmic transformation to the dependent variable did not help much as it resulted in a negatively skewed distribution.\n",
        "* Square root and cube root transformations were attempted, but they did not result in a normally distributed variable.\n",
        "* Therefore, we will use a square root transformation for the regression as it transformed the variable into a well-distributed form."
      ],
      "metadata": {
        "id": "8TL9wantTSr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2 , figsize = (15,5))\n",
        "\n",
        "#  checking square root tranformation in our target variable\n",
        "dist =sns.distplot(np.sqrt(data['rented_bike_count']), ax = ax[0])\n",
        "dist.set(xlabel = 'Rented Bike Count', ylabel ='Density', title = 'Distribution Plot of Target Variable in sqrt tranformation')\n",
        "\n",
        "# mean line\n",
        "dist.axvline(np.sqrt(data['rented_bike_count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "# median line\n",
        "dist.axvline(np.sqrt(data['rented_bike_count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "# Boxplot\n",
        "box = sns.boxplot(np.sqrt(data.rented_bike_count), ax= ax[1], orient = 'h')\n",
        "box.set(title = 'Outlier Analysis of Target Variable in sqrt tranformation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4cf_OFBUTTyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "\n",
        "* By applying the square root transformation to the skewed Rented Bike Count, we were able to obtain an almost normal distribution, which is in line with the general rule that skewed variables should be normalized in linear regression.\n",
        "* We find that there are no outliers in the Rented Bike Count column after applying square root transformation."
      ],
      "metadata": {
        "id": "0vYX7gxGTkFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manipulations done and insights found**\n",
        "1. We checked for correlation coefficient and found that most of the numerical features are positively correlated to our target variable.\n",
        "2. From heatmap and correlation coefficient, dew_point_temperature and temperature have a correlation coefficient of 0.91 and dew_point_temperature is less correlated to our target variable hence we dropped dew_point_temperature.\n",
        "We also did a VIF analysis to remove multi-colinearity and since the VIF factor of 'year' is too large hence we removed the year from our data to build our model.\n",
        "3. We encoded our categorical features which are necessary for the model to understand. We used one hot encoding for 'seasons' and Numeric encoding for 'holiday' and 'functioning_day'. Other columns are already encoded.\n",
        "4. To treat our target variable we Applied a logarithmic transformation to the target variable did not help much as it resulted in a negatively skewed distribution, Square root, and cube root transformations were attempted, but they did not result in a normally distributed variable. Therefore, we used a square root transformation for the regression as it transformed the variable into a well-distributed form."
      ],
      "metadata": {
        "id": "P4u2V7e0Tu71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "features = [i for i in data.columns if i not in ['rented_bike_count']]"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(data[features])"
      ],
      "metadata": {
        "id": "FTo6uFWAUKNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this we have different independent features of different scale so we have used standard scalar method to scale our independent features into one scale."
      ],
      "metadata": {
        "id": "U7xhrar_UPRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\\# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X = data.drop('rented_bike_count', axis=1)\n",
        "y= np.sqrt(data['rented_bike_count'])"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "asM5VEQVU0HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model we have split the data into train and test using train_test_split method\n",
        "\n",
        "We have split 80% of our data into train and 20% into test."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}